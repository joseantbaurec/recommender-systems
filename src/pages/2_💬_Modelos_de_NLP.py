import pandas as pd
import streamlit as st

import utils as colors
from base.plotting import plotly_from_json
from utils import bullet_item, highlight, setup_page, vspace, write

# page configs
setup_page()

# page content
st.title('Modelos basados en NLP')
vspace(3)
col1, _ = st.columns([8.5, 1])
with col1:
    write(
        'Para definir '
        + highlight('**similitud**', colors.HIGHLIGHT_GRAY)
        + ' podemos acercarnos al mundo del <u>*Natural Language Processing*</u> y obtener inspiraci칩n. Como hicimos en el modelo de *co-popularidad*, suponemos que '
        'dos productos son **similares** si reciben interacciones por un mismo usuario. Sin embargo, en vez de hacer un simple conteo, lo miramos desde el punto de vista '
        'de la *frecuencia en contexto*.'
    )
    write(
        'Supongamos que los usuarios interact칰an con los productos en '
        + highlight('**sesiones**', colors.HIGHLIGHT_BLUE)
        + ', franjas de tiempo en las que todas las interacciones que realiza un mismo usuario son *consecutivas*, o comparten alg칰n tipo de *contexto com칰n*. Por ejemplo, '
        'un usuario puede ver seguidos dos capitulos de una misma *sitcom*, y al d칤a siguiente ver tres documentales sobre ballenas; no quiere decir que los documentales de '
        'ballenas sean similares a las *sitcoms*.'
    )
    write(
        'Los modelos cl치sicos en NLP de <u>*word embeddings*</u> son muy 칰tiles en estos casos: los *productos* se convierten en '
        + highlight('**palabras**', colors.HIGHLIGHT_GRAY)
        + ', mientras que las *sesiones* se convierten en '
        + highlight('**frases**', colors.HIGHLIGHT_GRAY)
        + '. As칤, estudiar la frecuencia y orden de los productos dentro de las sesiones nos proporciona un '
        + highlight('**embedding**', colors.HIGHLIGHT_GREEN)
        + ' del banco de productos en un espacio de dimensi칩n "peque침a", donde las palabras est치n m치s cerca cuanto m치s frecuentemente aparecen juntas en una sesi칩n.'
    )

vspace(3)
col1, _, col2, _ = st.columns([6, 0.5, 4.5, 1])
with col1:
    st.header('游눫 Recomendador Word2Vec 游늻')
    write(
        'Uno de los algoritmos m치s cl치sicos de *word embeddings* es **Word2Vec**. Se trata de una red neuronal con una sola capa densa oculta (el *embedding*), y tanto las capas de '
        'input como de output representan el *one-hot encoding* del conjunto de palabras.'
    )
    write(
        'Dos modalidades existen para este modelo, **Skip-Gram** y **Continous Bag of Words**: *Skip-Gram* nos da la probabilidad de que cada palabra aparezca "cerca" de una palabra dada, '
        'mientras que *CBoW* nos da la probabilidad de que cada palabra aparezca "en medio" de una frase dada. Ambas pueden ser 칰tiles, pero la primera modalidad suele ser la m치s utilizada.'
    )
    write(
        'Gracias a la implementaci칩n, podemos hacer **consultas agregadas**: buscar a partir del historial de un usuario, en vez de utilizar un 칰nico producto como *prompt* para el modelo.'
    )
with col2:
    vspace(5)
    st.image('res/images/word2vec.png')
vspace(2)
col1, _ = st.columns([8.5, 1])
with col1:
    write(
        'Aunque es dif칤cil de navegar, podemos visualizar y explorar el *embedding* que aprende Word2Vec en tres dimensiones a trav칠s de la siguiente gr치fica:'
    )
    fig = plotly_from_json('res/figures/word2vec_embedding.json')
    fig.update_layout(width=1225, height=750)
    fig.update_traces(marker_line_width=0, opacity=0.5)
    st.plotly_chart(fig)
col1, _ = st.columns([8.5, 1])
with col1:
    st.header('游빑 Estad칤sticas 游눮')
    vspace(1)
    write('Evaluemos nuestro modelo con las mismas m칠tricas base anteriores:')
    vspace(1)

_, col1, _, col2, _ = st.columns([0.1, 3, 0.1, 3, 0.7])
with col1:
    write('<u>*Entrenamiento*</u>')
    metrics = [
        ['1h 40min', '-', '-', '-'],
        ['0.002s', '0.002s', '0.035s', '0.002s'],
        ['0.0591', '0.0200', '0.5911', '0'],
        ['0.0752', '0.0445', '0.4545', '0'],
        ['12.10%', '10%', '70%', '0%'],
        ['59.00%', '100%', '100%', '0%'],
        ['3.7458', '3', '10', '-'],
        ['0.2597', '0.1429', '1', '0'],
    ]
    cols = ['Media', 'Mediana', 'M치s alto', 'M치s bajo']
    indx = [
        'T. Entrenamiento',
        'T. Evaluaci칩n',
        'MAP@k',
        'R@k',
        'P@k',
        'HR@k',
        'Rango@k',
        'RangoRec@k',
    ]
    metrics = pd.DataFrame(metrics, columns=cols, index=indx)
    st.dataframe(metrics, use_container_width=True)
with col2:
    write('<u>*Validaci칩n*</u>')
    metrics = [
        ['-', '-', '-', '-'],
        ['-', '-', '-', '-'],
        ['0.0238', '0', '0.5', '0'],
        ['0.0510', '0', '0.5', '0'],
        ['1.50%', '0%', '20%', '0%'],
        ['13.00%', '0%', '100%', '0%'],
        ['3.9231', '4', '9', '-'],
        ['0.0607', '0', '1', '0'],
    ]
    cols = ['Media', 'Mediana', 'M치s alto', 'M치s bajo']
    indx = [
        'T. Entrenamiento',
        'T. Evaluaci칩n',
        'MAP@k',
        'R@k',
        'P@k',
        'HR@k',
        'Rango@k',
        'RangoRec@k',
    ]
    metrics = pd.DataFrame(metrics, columns=cols, index=indx)
    st.dataframe(metrics, use_container_width=True)


col1, _ = st.columns([8.5, 1])
with col1:
    st.header('游닇 Conclusiones 游눫')
    write(
        'Al igual que el modelo de **co-popularidad**, una gran parte del tiempo de entrenamiento se invierte en construir el conjunto de sesiones, ya que nuestro dataset no ven칤a preparado '
        'para ello. A칰n as칤, este tiempo vuelve a ser un *one-off*, y ampliar con nuevas sesiones no es tan costoso. De hecho, al ser el modelo una red neuronal de una sola capa, podemos '
        'retomar el entrenamiento en cualquier momento si a침adimos m치s sesiones. Con esto conseguimos que el modelo '
        + highlight('**escale**', colors.HIGHLIGHT_GREEN)
        + ' conforme a침adimos m치s sesiones.'
    )
    write(
        'Te칩ricamente, Word2Vec tambi칠n sufre de '
        + highlight('**popularity bias**', colors.HIGHLIGHT_RED)
        + ': los productos populares apareceran con frecuencia "cerca" de cualquier otro producto. Sin embargo, podemos aprovechar un hiperpar치metro llamado '
        + highlight('**ventana**', colors.HIGHLIGHT_BLUE)
        + ', que controla cu치nto nos podemos alejar de la palabra central hasta dejar de considerarlo "parte del contexto". Si bien esto no los libera por todo del *bias*, conseguimos '
        'no recomendar productos populares pero que no tienen nada que ver con el *prompt*.'
    )
    write(
        'Este modelo es un claro ejemplo de <u>*Collaborative Filtering*</u>: hemos aprendido qu칠 productos son similares a partir de sus interacciones con los usuarios. Sin embargo, '
        'notamos si bien aprende del conjunto de usuarios, no es capaz de ajustarse a ninguno en particular.'
    )
    write(
        'Adem치s, a pesar de ser capaz de poder ampliar el "vocabulario" seg칰n sea necesario, no es capaz de dar recomendaciones para un producto que a칰n no conoce: sufre de '
        + highlight('**Cold Start**', colors.HIGHLIGHT_RED)
        + ' del lado de los productos, pero no de los usuarios. Por tanto, este modelo es 칰til para recomendaciones "an칩nimas", pero fundadas.'
    )
